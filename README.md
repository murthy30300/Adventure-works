# ğŸ”· Azure End-to-End Data Pipeline Project

This project demonstrates a complete **cloud-native data pipeline** built using Microsoft Azure services. It follows the modern **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) and showcases how raw external data can be ingested, transformed, queried, and visualized â€” all on the cloud.

---

## ğŸ“Œ Project Highlights

- **Data Ingestion** using **Azure Data Factory** from external HTTP sources (e.g., GitHub)
- **Data Storage** in **Azure Data Lake Storage Gen2** with Bronze, Silver, and Gold layers
- **Data Transformation** using **Azure Databricks + PySpark**
- **Data Modeling and SQL querying** with **Azure Synapse Analytics**
- **Business Intelligence and Dashboards** via **Power BI**

---

## ğŸ“½ï¸ Project Demo

ğŸ”— **Watch the screen recording demo on LinkedIn:**  
[â–¶ Video Post Link](https://www.linkedin.com/posts/vishnu1702_azure-dataengineering-databricks-activity-7317900100133953536-ct9N?utm_source=share&utm_medium=member_desktop&rcm=ACoAAEZOv90BlVXEz6AoDf0TykCGkpsmrXtdjGg)

ğŸ“¥ **Embedded Video:**  
```html
<iframe src="https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:7317899971708493825?compact=1" height="399" width="504" frameborder="0" allowfullscreen="" title="Embedded post"></iframe>
```

## âœï¸ Article Walkthrough

ğŸ“„ **Read the full technical article:**  
[ğŸ“š LinkedIn Article Post](https://www.linkedin.com/posts/vishnu1702_azure-data-engineering-activity-7317909241191612416-1d9L?utm_source=share&utm_medium=member_desktop)

ğŸ“° **Embedded Article View:**  
```html
<iframe src="https://www.linkedin.com/embed/feed/update/urn:li:ugcPost:7317909241191612416?collapsed=1" height="399" width="504" frameborder="0" allowfullscreen="" title="Embedded post"></iframe>
```

## ğŸ§± Architecture Overview

```
[HTTP Source (GitHub)] 
           â†“
[Azure Data Factory (ADF)] 
           â†“
[Bronze Layer - Raw Data Storage] 
           â†“
[Azure Databricks - Data Transformation (PySpark)] 
           â†“
[Silver Layer - Cleaned Data Storage] 
           â†“
[Azure Synapse Analytics - Querying & Modeling] 
           â†“
[Gold Layer - Business Ready Data]
           â†“
[Power BI - Dashboards and Insights]
```

## ğŸ’¡ Technologies Used

- Azure Data Factory
- Azure Data Lake Storage Gen2
- Azure Databricks (PySpark)
- Azure Synapse Analytics
- Power BI
- GitHub (as HTTP data source)
- Entra ID (for secure access configuration)

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ adf/                          # Azure Data Factory pipelines and configurations
â”‚   â”œâ”€â”€ pipelines/                # Data ingestion pipeline definitions
â”‚   â””â”€â”€ linked_services/          # Connection configurations
â”‚
â”œâ”€â”€ databricks/                   # Databricks notebooks
â”‚   â”œâ”€â”€ bronze_to_silver/         # Transformation scripts for raw to cleaned data
â”‚   â””â”€â”€ silver_to_gold/           # Transformation scripts for analytics-ready data
â”‚
â”œâ”€â”€ synapse/                      # Synapse Analytics SQL scripts
â”‚   â”œâ”€â”€ views/                    # SQL views for data modeling
â”‚   â””â”€â”€ stored_procedures/        # Stored procedures for data operations
â”‚
â”œâ”€â”€ powerbi/                      # Power BI report templates
â”‚   â””â”€â”€ dashboards/               # Business intelligence dashboard designs
â”‚
â””â”€â”€ docs/                         # Project documentation
    â”œâ”€â”€ architecture.md           # Detailed architecture explanation
    â””â”€â”€ setup_guide.md            # Deployment and configuration instructions
```

## ğŸš€ Getting Started

### Prerequisites

- Azure subscription with contributor access
- Azure CLI or Azure PowerShell installed locally
- Basic knowledge of SQL and PySpark

### Deployment Steps

1. **Resource Provisioning**
   ```bash
   # Clone this repository
   git clone https://github.com/murthy30300/Adventure-works.git
   cd azure-data-pipeline
   
   # Deploy Azure resources using ARM template
   az deployment group create --resource-group YourResourceGroup --template-file deploy/azuredeploy.json
   ```

2. **Configure Data Factory**
   - Import the ADF pipelines from the `/adf` directory
   - Configure linked services with your storage account and key vault information

3. **Deploy Databricks Notebooks**
   - Import notebooks from the `/databricks` directory to your Databricks workspace
   - Configure cluster with required libraries (PySpark, Delta Lake)

4. **Set Up Synapse Analytics**
   - Execute SQL scripts from the `/synapse` directory
   - Configure access permissions for your data sources

5. **Configure Power BI**
   - Open the template files from the `/powerbi` directory
   - Connect to your Synapse Analytics workspace



## ğŸ“¬ Let's Connect

Feel free to [connect or reach out on LinkedIn](https://www.linkedin.com/in/vishnu1702/) to discuss the project or explore collaboration opportunities!

